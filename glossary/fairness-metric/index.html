<!doctype html><html lang="en"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width,initial-scale=1" /><title>Fairness Metric &#x2013; Glossary &#x2013; PulseGeek</title><meta name="description" content="A quantitative measure used to assess whether an AI model treats groups fairly, often comparing error rates, selection rates, or outcomes across protected groups." /><link rel="canonical" href="https://pulsegeek.com/glossary/fairness-metric" /><link rel="apple-touch-icon" sizes="180x180" href="https://pulsegeek.com/apple-touch-icon.png" /><link rel="icon" type="image/png" sizes="32x32" href="https://pulsegeek.com/favicon-32x32.png" /><link rel="icon" type="image/png" sizes="16x16" href="https://pulsegeek.com/favicon-16x16.png" /><link rel="manifest" href="https://pulsegeek.com/site.webmanifest" /><link rel="alternate" type="application/rss+xml" title="PulseGeek RSS feed" href="https://pulsegeek.com/rss.xml" /><link rel="alternate" type="application/atom+xml" title="PulseGeek Atom feed" href="https://pulsegeek.com/atom.xml" /><link rel="alternate" type="application/feed+json" title="PulseGeek JSON feed" href="https://pulsegeek.com/feed.json" /><meta property="og:title" content="Fairness Metric &#x2013; Glossary &#x2013; PulseGeek" /><meta property="og:type" content="website" /><meta property="og:url" content="https://pulsegeek.com/glossary/fairness-metric" /><meta property="og:image" content="https://pulsegeek.com/images/logo.png" /><meta property="og:description" content="A quantitative measure used to assess whether an AI model treats groups fairly, often comparing error rates, selection rates, or outcomes across protected groups." /><meta property="og:site_name" content="PulseGeek" /><meta property="og:locale" content="en_US" /><meta name="twitter:card" content="summary_large_image" /><meta name="twitter:title" content="Fairness Metric &#x2013; Glossary &#x2013; PulseGeek" /><meta name="twitter:description" content="A quantitative measure used to assess whether an AI model treats groups fairly, often comparing error rates, selection rates, or outcomes across protected groups." /><meta name="twitter:image" content="https://pulsegeek.com/images/logo.png" /><script type="application/ld+json"> {"@context":"https://schema.org","@graph":[{"@type":"DefinedTerm","@id":"https://pulsegeek.com/glossary/fairness-metric#term","name":"Fairness Metric","description":"A quantitative measure used to assess whether an AI model treats groups fairly, often comparing error rates, selection rates, or outcomes across protected groups.","inDefinedTermSet":"https://pulsegeek.com/glossary/","synonyms":["bias metric","equity metric","fairness measure"]},{"@type":"WebPage","@id":"https://pulsegeek.com/glossary/fairness-metric#page","name":"Fairness Metric","url":"https://pulsegeek.com/glossary/fairness-metric","isPartOf":{"@id":"https://pulsegeek.com#website"}},{"@type":"Organization","@id":"https://pulsegeek.com#organization","url":"https://pulsegeek.com","name":"PulseGeek","logo":{"@type":"ImageObject","url":"https://pulsegeek.com/images/logo.png"}},{"@type":"WebSite","@id":"https://pulsegeek.com#website","url":"https://pulsegeek.com","name":"PulseGeek"}]} </script><link href="https://pulsegeek.com/css/pico.green.min.css" rel="stylesheet" /><link href="https://pulsegeek.com/css/site.css" rel="stylesheet" /></head><body><header class="site-header"><div class="container container-narrow"><nav><ul><li><a href="https://pulsegeek.com/" class="brand" aria-label="PulseGeek home"><img src="https://pulsegeek.com/images/logo.png" srcset="https://pulsegeek.com/images/logo.png 1x, https://pulsegeek.com/images/logo@2x.png 2x" alt="PulseGeek" width="308" height="64" class="brand-logo" decoding="async" fetchpriority="high"></a></li></ul><ul><li><a href="https://pulsegeek.com/technology/">Technology</a></li></ul></nav></div></header><main class="container"><nav aria-label="Breadcrumb" class="breadcrumb"><ol><li class="breadcrumb-item" style="max-width: 180px; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://pulsegeek.com/" title="Home">Home</a></li><li class="breadcrumb-item" style="max-width: 180px; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://pulsegeek.com/glossary/" title="Glossary">Glossary</a></li><li class="breadcrumb-item" style="max-width: 180px; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><span>Fairness Metric</span></li></ol></nav><section class="container"><header><h1>Fairness Metric</h1><p class="lede">A quantitative measure used to assess whether an AI model treats groups fairly, often comparing error rates, selection rates, or outcomes across protected groups.</p></header><article class="term-body"><h2>In detail</h2><p>A fairness metric is a numerical way to evaluate if an AI system treats individuals or groups fairly. Common examples include demographic parity, which looks at equal selection rates, and equalized odds, which compares error rates across groups. The right metric depends on context, harms, and legal constraints. Because fairness goals can conflict, teams should choose metrics with stakeholders, document trade-offs, and test during development and after launch. Metrics guide decisions but do not replace judgment, monitoring, or clear governance. They work best when paired with bias mitigation techniques and transparent reporting.</p></article><aside class="term-meta"><div><h3>Synonyms</h3><p>bias metric, equity metric, fairness measure</p></div><div><h3>Related terms</h3><ul><li><a href="https://pulsegeek.com/glossary/algorithmic-bias/">Algorithmic Bias</a></li><li><a href="https://pulsegeek.com/glossary/demographic-parity/">Demographic Parity</a></li><li><a href="https://pulsegeek.com/glossary/equalized-odds/">Equalized Odds</a></li><li><a href="https://pulsegeek.com/glossary/explainable-ai/">Explainable AI</a></li></ul></div></aside><section class="related-articles"><h2>Related articles</h2><ul><li><a href="https://pulsegeek.com/articles/best-tools-for-ai-dataset-versioning-and-lineage/">Best Tools for AI Dataset Versioning and Lineage</a></li><li><a href="https://pulsegeek.com/articles/how-to-conduct-a-data-bias-audit-with-confidence/">How to Conduct a Data Bias Audit with Confidence</a></li><li><a href="https://pulsegeek.com/articles/how-to-build-an-ai-ethics-policy-that-scales/">How to Build an AI Ethics Policy That Scales</a></li><li><a href="https://pulsegeek.com/articles/ai-governance-framework-components-a-working-guide/">AI Governance Framework Components: A Working Guide</a></li><li><a href="https://pulsegeek.com/articles/ai-ethics-and-fairness-practical-paths-to-responsible-ai/">AI Ethics and Fairness: Practical Paths to Responsible AI</a></li></ul></section></section></main><footer class="container" itemscope itemtype="https://schema.org/Organization"><hr /><!-- — Site-wide nav links (SEO-friendly) — --><nav aria-label="Footer navigation" itemscope itemtype="https://schema.org/SiteNavigationElement"><ul style="list-style:none; padding-left:0; margin:0; display:flex; flex-wrap:wrap; gap:.65rem;"><li itemprop="name"><a href="https://pulsegeek.com/about/" itemprop="url">About</a></li><li itemprop="name"><a href="https://pulsegeek.com/contact/" itemprop="url">Contact</a></li><li itemprop="name"><a href="https://pulsegeek.com/privacy/" itemprop="url">Privacy&nbsp;Policy</a></li><li itemprop="name"><a href="https://pulsegeek.com/terms/" itemprop="url">Terms&nbsp;of&nbsp;Service</a></li><li itemprop="name"><a href="https://pulsegeek.com/site-map/" itemprop="url">HTML&nbsp;Sitemap</a></li><li itemprop="name"><a href="https://pulsegeek.com/rss.xml" itemprop="url" title="RSS 2.0 feed">RSS&nbsp;Feed</a></li><li itemprop="name"><a href="https://pulsegeek.com/atom.xml" itemprop="url" title="Atom 1.0 feed">Atom</a></li><li itemprop="name"><a href="https://pulsegeek.com/feed.json" itemprop="url" title="JSON Feed 1.1">JSON&nbsp;Feed</a></li></ul></nav><!-- — Copyright — --><small style="display:block; margin-top:.75rem;"> © 2025 <span itemprop="name">PulseGeek</span>. All rights reserved. </small></footer></body></html> 